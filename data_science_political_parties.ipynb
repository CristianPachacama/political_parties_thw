{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CristianPachacama/political_parties_thw/blob/main/data_science_political_parties.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiXrkRoHL2p-"
      },
      "source": [
        "# Instructions\n",
        "\n",
        "## Survey Analysis\n",
        "\n",
        "As a data scientist you are required to analyse the political landscape of Europe using the **Chapel Hill Expert Survery** dataset. The dataset provides insights into the positioning of **277 political parties** in Europe based on **55 different attributes**. The dataset can be downloaded [here](https://www.chesdata.eu/2019-chapel-hill-expert-survey) and the [codebook](https://static1.squarespace.com/static/5975c9bfdb29d6a05c65209b/t/5fa04ec05d3c8218b7c91450/1604341440585/2019_CHES_codebook.pdf) provides further information on the survey attributes.\n",
        "\n",
        "This repository contains the necessary setup and code base to help guide you in performing an analysis using different statistical methods.\n",
        "\n",
        "## Project Setup\n",
        "\n",
        "### Pre-requisites\n",
        "\n",
        "Please make sure you have a Google account in order to use the [Google Colaboratory](https://colab.research.google.com/).\n",
        "\n",
        "### Install all python dependencies\n",
        "\n",
        "Google Colaboratory by default already has some of the most commom dependencies used by data scientists. In case you need install some additional dependencies you can do it by runing in an empty cell something like\n",
        "\n",
        "```bash\n",
        "!pip install <python-package-name>\n",
        "```\n",
        "\n",
        "## Gearing Up for the Pairing Session\n",
        "\n",
        "Please be sure to complete the below tasks before the pairing session.\n",
        "\n",
        "1. Get a high-level understanding of the dataset by looking into the [codebook](https://static1.squarespace.com/static/5975c9bfdb29d6a05c65209b/t/5fa04ec05d3c8218b7c91450/1604341440585/2019_CHES_codebook.pdf) and if necessary downloading the dataset.\n",
        "2. Have your coding environment ready by installing python and poetry.\n",
        "3. Ensure that you are able to run all commands mentioned in this README (except for pytest errors)\n",
        "\n",
        "**Please note that you DO NOT have to complete the code/tasks in this notebook. It is meant to be done together during pairing session.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqghYpfLZCEn"
      },
      "source": [
        "---\n",
        "\n",
        "# Import modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPAohhAzZCeT"
      },
      "outputs": [],
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "from itertools import cycle\n",
        "from urllib.request import urlretrieve\n",
        "import logging\n",
        "\n",
        "import sqlite3\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "from matplotlib import pyplot\n",
        "\n",
        "from sklearn import decomposition, mixture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCcYSwgVmvuQ"
      },
      "outputs": [],
      "source": [
        "logger = logging.getLogger('my_logger')\n",
        "logging.basicConfig(level=logging.DEBUG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-RbmDwmOXXl"
      },
      "source": [
        "# 0. Data loading\n",
        "Here we implemented two classes: `DataExtractor` and `EuropeanParties`.\n",
        "\n",
        "The `DataExtractor` class downloads the data, whilst the `EuropeanParties` class was implemented to manipulate the raw table, named as `raw_table` in the SQLite DB.\n",
        "\n",
        "After loading the data on the DataBase, you are required perform some descritive statistics using the `raw data`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hstmnunnOdTN"
      },
      "outputs": [],
      "source": [
        "class DataExtractor:\n",
        "    \"\"\"A class to extract data from internet\"\"\"\n",
        "    data_url: str = \"https://cadmus.eui.eu/bitstream/handle/1814/69975/CHES_TREND_1999-2019.dta?sequence=6&isAllowed=y\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.party_data = self._download_data()\n",
        "        self.non_features = []\n",
        "        self.index = [\"party_id\", \"party\", \"country\"]\n",
        "\n",
        "    def _download_data(self) -> pd.DataFrame:\n",
        "\n",
        "        logging.debug(f\"Extracting data from {self.data_url}\")\n",
        "        Path(\"data\").mkdir(exist_ok=True)\n",
        "        data_path, _ = urlretrieve(\n",
        "          self.data_url,\n",
        "          Path(\"data\").joinpath(*[\"CHES2019V3.dta\"]),\n",
        "        )\n",
        "        raw_data =  pd.read_stata(data_path)\n",
        "        clean_raw_data = self._clean_data(raw_data)\n",
        "        for c in list(clean_raw_data.dtypes[clean_raw_data.dtypes == 'category'].index):\n",
        "            clean_raw_data[c] = clean_raw_data[c].astype(str)\n",
        "        logging.debug(\"Data extracted...\")\n",
        "\n",
        "        return clean_raw_data\n",
        "\n",
        "    def _clean_data(self, df: pd.DataFrame):\n",
        "        df = df.copy()\n",
        "        mapper = {\"eastwest_map\": {\n",
        "              'east': 1,\n",
        "              'west': 0\n",
        "            },\n",
        "            \"eumember_map\": {\n",
        "                'other': 0,\n",
        "                'EU member state': 1\n",
        "            },\n",
        "            \"eu_position_map\": {\n",
        "              \"strongly opposed\": 1,\n",
        "              \"opposed\": 2,\n",
        "              \"somewhat opposed\": 3,\n",
        "              \"neutral\": 4,\n",
        "              \"somewhat in favor\": 5,\n",
        "              \"in favor\": 6,\n",
        "              \"strongly in favor\": 7\n",
        "            },\n",
        "            \"eu_intmark_map\": {\n",
        "                \"neutral toward expanding EU powers on the internal market\": 3,\n",
        "                \"stongly favors explanding EU powers on the internal market\": 7,\n",
        "                \"stongly opposes expanding EU powers on the internal market\": 1\n",
        "                },\n",
        "            \"eu_cohesion_map\": {\n",
        "                \"neutral towards the EU's cohesion policy\": 3,\n",
        "                \"strongly favors the EU's cohesion policy\": 7,\n",
        "                \"strongly opposes the EU's cohesion policy\": 1\n",
        "                },\n",
        "            \"eu_asylum_map\": {\n",
        "                \"neutral towards a common policy on political asylum\": 3,\n",
        "                \"strongly favors a common policy on political asylum\": 7,\n",
        "                \"strongly opposes a common policy on political asylum\": 1\n",
        "                },\n",
        "            \"eu_foreign_map\": {\n",
        "                \"neutral towards a common foreign and security policy\": 3,\n",
        "                \"strongly favors a common foreign and security policy\": 7,\n",
        "                \"strongly opposes a common foreign and security policy\": 1\n",
        "                },\n",
        "            \"lrgen_map\": {\n",
        "                \"extreme left\": 0,\n",
        "                'center': 5,\n",
        "                'extreme right': 10,\n",
        "            },\n",
        "            \"lrecon_map\": {\n",
        "                \"extreme left\": 0,\n",
        "                'center': 5,\n",
        "                'extreme right': 10,\n",
        "            },\n",
        "            \"galtan_map\": {\n",
        "                'center': 5,\n",
        "                'extreme tan': 10,\n",
        "                },\n",
        "            \"multicult_salience_map\": {\n",
        "                \"extremely important\": 10}}\n",
        "\n",
        "        exclude = ['family', 'govt', 'eu_ep', 'eu_fiscal', 'eu_employ', 'eu_agri', 'eu_environ', 'eu_turkey', 'mip_one', 'mip_two', 'mip_three', 'multicult_sal', '_mergexxx', 'chesversion', 'year', 'eumember', 'expert']\n",
        "\n",
        "        for k, m in mapper.items():\n",
        "            col = k.replace('_map', '')\n",
        "            df[col] = df[col].replace(m)\n",
        "            df[col] = df[col].astype(float)\n",
        "\n",
        "        df = df[df['year'] == 2019]\n",
        "        all_nan_values = pd.isnull(df).sum()\n",
        "        exclude_all_nan_cols = list(all_nan_values[all_nan_values == df.shape[0]].index)\n",
        "        include = [col for col in df.columns if col not in (exclude + exclude_all_nan_cols)]\n",
        "        df = df[include]\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDCslTK_ZVbX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1PK_zZ2Ofzd"
      },
      "outputs": [],
      "source": [
        "class EuropeanParties:\n",
        "    \"\"\" Database connector class\"\"\"\n",
        "    def __init__(self):\n",
        "        self._conn = self._create_connection()\n",
        "        self._cursor = self._conn.cursor()\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        self.close()\n",
        "\n",
        "    def _create_connection(self):\n",
        "        import math\n",
        "        conn =  sqlite3.connect(\"data/political-parties.db\")\n",
        "        conn.create_function('SQRT', 1, math.sqrt)\n",
        "        conn.create_aggregate('VAR', 1, Variance)\n",
        "\n",
        "        return conn\n",
        "\n",
        "    def close(self):\n",
        "        self._conn.close()\n",
        "\n",
        "    def create_and_insert_table(self, dataframe: pd.DataFrame, table_name: str):\n",
        "        dataframe.to_sql(table_name, self._conn, if_exists=\"replace\", index=False)\n",
        "\n",
        "    def query(self, query: str) -> pd.DataFrame:\n",
        "        self._cursor.execute(query)\n",
        "        column_names = list(map(lambda x: x[0], self._cursor.description))\n",
        "        data = self._cursor.fetchall()\n",
        "        return pd.DataFrame(data, columns=column_names)\n",
        "\n",
        "class Variance:\n",
        "    def __init__(self):\n",
        "        self.n = 0\n",
        "        self.sum = 0\n",
        "        self.sum_squared = 0\n",
        "\n",
        "    def step(self, value):\n",
        "        try:\n",
        "            if value is None:\n",
        "                return None\n",
        "\n",
        "            self.n += 1\n",
        "            self.sum += value\n",
        "            self.sum_squared += (value ** 2)\n",
        "        except Exception as steperr:\n",
        "            pass\n",
        "            return None\n",
        "\n",
        "    def finalize(self):\n",
        "        avg_x_squared = self.sum_squared / self.n\n",
        "        avg_x = self.sum / self.n\n",
        "        variance = (avg_x_squared - (avg_x ** 2)) * (self.n / (self.n - 1))\n",
        "        return variance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAGToHCXHbCu"
      },
      "source": [
        "## 0.1. Extracting data and inserting in a table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBsiHkcqOmk9"
      },
      "outputs": [],
      "source": [
        "extractor = DataExtractor()\n",
        "raw_data = extractor.party_data\n",
        "with EuropeanParties() as db:\n",
        "    db.create_and_insert_table(raw_data, \"raw_data\")\n",
        "\n",
        "all_data_query = \"\"\"\n",
        "    SELECT *\n",
        "    FROM raw_data\n",
        "\"\"\"\n",
        "with EuropeanParties() as db:\n",
        "    data = db.query(all_data_query)\n",
        "display(data.head())\n",
        "\n",
        "assert data.shape == raw_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTdY1NTDHbCw"
      },
      "source": [
        "## 0.2 Raw data descritive statistics\n",
        "> Show some descriptive statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpB77nx0HbCw"
      },
      "outputs": [],
      "source": [
        "##### YOUR CODE GOES HERE #####\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6P_Lh1aNInT"
      },
      "source": [
        "# 1. Data processing\n",
        "\n",
        "In this task you are required to implement the methods in the class `DataLoader` for processing the data. These are the required steps of data processing you must write code for (using SQL):  \n",
        "\n",
        ">**1.1. Duplicate rows removal**  \n",
        ">\n",
        ">   Remove rows in which have all elements are equal to each other leaving only one of them in the dataset.   \n",
        ">\n",
        ">**1.2. Handle NaN values.**\n",
        ">\n",
        ">   Fill the null values with zeros.\n",
        ">\n",
        ">**1.3. Scaling data.**  \n",
        ">\n",
        ">   Scale all the features in order for them to have zero mean and unit variance, i.e. $\\overline{x} = 0$  and $\\sigma^{2} = 1$.\n",
        ">\n",
        ">**1.4. Non-feature columns removal and dataframe index setting.**  \n",
        ">   Remove a list of columns, passed as argument for the function/method, from the dataset. Additionaly, set another list of columns, also passed as argument for the function or method, as index of the dataframe.  \n",
        ">\n",
        "Finally after implementing the steps above put them all together inside the `preprocess_data` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzQoAc7tqNMQ"
      },
      "outputs": [],
      "source": [
        "class DataLoader:\n",
        "    \"\"\"Class to load the political parties dataset\"\"\"\n",
        "    def __init__(self, connector: EuropeanParties, table_columns: List, numeric_features: List):\n",
        "        self.connector = connector\n",
        "        self.table_columns = table_columns\n",
        "        self.non_features = []\n",
        "        self.numeric_features = numeric_features\n",
        "        self.index = [\"party_id\", \"party\", \"country\"]\n",
        "        self.non_numeric_columns = [col for col in table_columns if col not in numeric_features]\n",
        "\n",
        "\n",
        "    def _preprocess_query(self) -> str:\n",
        "        \"\"\"Write a function to combine the drop duplicates,\n",
        "        handle nan values and scale feature queries.\"\"\"\n",
        "        ##### YOUR CODE GOES HERE #####\n",
        "        pass\n",
        "\n",
        "    def _drop_duplicates_query(self, table_name: str) -> str:\n",
        "        \"\"\"Write a function to remove duplicates in the DataBase table\"\"\"\n",
        "        ##### YOUR CODE GOES HERE #####\n",
        "        pass\n",
        "\n",
        "    def _scale_features_query(self, table_name: str) -> str:\n",
        "        \"\"\"Scale features to have zero mean and unit variance.\n",
        "        This function must return a SQL query\"\"\"\n",
        "        # Note SQRT and VAR functions were implemented\n",
        "        ##### YOUR CODE GOES HERE #####\n",
        "        pass\n",
        "\n",
        "    def _handle_NaN_values_query(self, table_name: str) -> str:\n",
        "        \"\"\"Write a function to handle NaN values.\n",
        "        This function must return a SQL query\"\"\"\n",
        "        ##### YOUR CODE GOES HERE #####\n",
        "        pass\n",
        "\n",
        "    def remove_nonfeature_cols(\n",
        "        self, df: pd.DataFrame, non_features: List[str], index: List[str]\n",
        "    ) -> pd.DataFrame:\n",
        "        \"\"\"Write a function to remove certain features cols and set certain cols as\n",
        "        indices in a dataframe\"\"\"\n",
        "        ##### YOUR CODE GOES HERE #####\n",
        "        pass\n",
        "\n",
        "    def preprocess_data(self) -> pd.DataFrame:\n",
        "        \"\"\"Write a function to combine all pre-processing steps for the dataset\"\"\"\n",
        "        ##### YOUR CODE GOES HERE #####\n",
        "\n",
        "        ###############################\n",
        "        self.party_data = ...\n",
        "\n",
        "\n",
        "\n",
        "class TestDataLoader:\n",
        "    def __init__(self, connector: EuropeanParties, raw_data: pd.DataFrame, data_loader: DataLoader):\n",
        "        self.connector = connector\n",
        "        self.raw_data = raw_data\n",
        "        self.data_loader = data_loader\n",
        "\n",
        "    def check_drop_duplicates_query(self):\n",
        "        with self.connector() as db:\n",
        "            data = db.query(self.data_loader._drop_duplicates_query('raw_data'))\n",
        "        logging.debug(\"1. Checking drop duplicates query\")\n",
        "        assert data.shape == (247, 54)\n",
        "        logging.debug(\"1. Drop duplicates passed\")\n",
        "        # display(data)\n",
        "\n",
        "    def check_handle_NaN_values_query(self):\n",
        "        with self.connector() as db:\n",
        "            data = db.query(self.data_loader._handle_NaN_values_query('raw_data'))\n",
        "        logging.debug(\"2. Checking handle NaN values query\")\n",
        "        assert pd.isnull(data).sum().sum() == 0\n",
        "        logging.debug(\"2. Handle NaN values passed\")\n",
        "        # display(data)\n",
        "\n",
        "    def check_scale_features_query(self):\n",
        "        from numpy.testing import assert_allclose\n",
        "        logging.debug(\"3. Checking scale features query\")\n",
        "        with self.connector() as db:\n",
        "            data = db.query(self.data_loader._scale_features_query('raw_data'))\n",
        "        valid_cols = [col for col in data.columns if col != 'party_id']\n",
        "        assert_allclose(data[valid_cols].mean(numeric_only=True), 0, atol=1e-07)\n",
        "        logging.debug(\"3. Scale features passed\")\n",
        "        # display(data.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ht762vMfHbC1"
      },
      "outputs": [],
      "source": [
        "numeric_features = list([col for col in raw_data.select_dtypes(exclude=['object', \"category\"]).columns if col != 'party_id'])\n",
        "table_columns = list(raw_data.columns)\n",
        "data_loader = DataLoader(connector=EuropeanParties, table_columns=table_columns, numeric_features=numeric_features)\n",
        "\n",
        "test_data_loader = TestDataLoader(connector=EuropeanParties, raw_data=raw_data, data_loader=data_loader)\n",
        "test_data_loader.check_drop_duplicates_query()\n",
        "test_data_loader.check_handle_NaN_values_query()\n",
        "test_data_loader.check_scale_features_query()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW8mu4K2HbC2"
      },
      "source": [
        "## 1.1. Data processing and descriptive statistics\n",
        "> Run the data processing and then show some descriptive statistics for the processed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gi25wVmUn0i4"
      },
      "outputs": [],
      "source": [
        "numeric_features = list([col for col in raw_data.select_dtypes(exclude=['object', \"category\"]).columns if col != 'party_id'])\n",
        "table_columns = list(raw_data.columns)\n",
        "data_loader = DataLoader(connector=EuropeanParties, table_columns=table_columns, numeric_features=numeric_features)\n",
        "data_loader.preprocess_data()\n",
        "sanitized_data = data_loader.party_data\n",
        "##### YOUR CODE GOES HERE #####\n",
        "\n",
        "###############################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3-PvEzuHbC3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otVF_nPXqDHN"
      },
      "source": [
        "# 2. Dimensionality reduction\n",
        "\n",
        "Now you will need to implement the class `DimensionalityReducer` in order to project the data onto lower dimensional spaces.\n",
        "\n",
        "Additionaly, implement the `scatter_plot` function to visualize the datapoints projected over $\\mathbb{R}^{2}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSwq1rf8Hu4O"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(level=logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roVJlUc3qMGF"
      },
      "outputs": [],
      "source": [
        "class DimensionalityReducer:\n",
        "    \"\"\"Class to model a dimensionality reduction method for the given dataset.\n",
        "    1. Write a function to convert the high dimensional data to 2 dimensional.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data: pd.DataFrame, n_components: int = 2):\n",
        "        self.n_components = n_components\n",
        "        self.data = data\n",
        "        self.feature_columns = data.columns\n",
        "        self.model = self._model()\n",
        "\n",
        "    ##### YOUR CODE GOES HERE #####\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmk_UVW5cQoi"
      },
      "outputs": [],
      "source": [
        "def scatter_plot(\n",
        "    transformed_data: pd.DataFrame,\n",
        "    x: str,\n",
        "    y: str,\n",
        "    color: str,\n",
        "    splot: pyplot.subplot = None,\n",
        "    labels: Optional[List[str]] = None\n",
        "):\n",
        "    \"\"\"Write a function to generate a 2D scatter plot.\"\"\"\n",
        "\n",
        "    ##### YOUR CODE GOES HERE #####\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XvTzIuMREHj"
      },
      "source": [
        "## 2.1. Dimensionality reduction\n",
        "\n",
        "#### 2.1.1. Model training  \n",
        "\n",
        "> Run the dimensionality reduction model training and then obtain the projected dataset.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOn58P7-Q8Pn"
      },
      "outputs": [],
      "source": [
        "##### YOUR CODE GOES HERE #####\n",
        "\n",
        "###############################\n",
        "\n",
        "reduced_dim_data = dim_reducer.transform(data_loader.party_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q20pKEwuHbC4"
      },
      "source": [
        "> Plot the projected data on a scatter plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1oGA67eHbC4"
      },
      "outputs": [],
      "source": [
        "##### YOUR CODE GOES HERE #####\n",
        "\n",
        "###############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxNDPiVQHbC4"
      },
      "source": [
        "> Now plot the projeted data so as to left and right parties have differnent colors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsjzpCjxHbC4"
      },
      "outputs": [],
      "source": [
        "##### YOUR CODE GOES HERE #####\n",
        "\n",
        "###############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RgsFXlWaBuf"
      },
      "source": [
        "# 3. Density estimation\n",
        "\n",
        "In this step you are going to implement the class `DensityEstimator`. The main goal here is being able to find an estimate of the unobserved underlying probability density function from the data. The class must also contain a method for generating theoretical parties, _i.e._ parties data generated by the density estimator.\n",
        "\n",
        "Note that the dimensionality reduction model is passed as an argument of this class. You must estimate the density function for the lower-dimentional data. Further in the test you will need to sample data from the estimated density function and then map it back to its original space.\n",
        "\n",
        "Furthermore, for the `plot_density_estimation_results` - used for visualizing the estimated density's region of confidence -  the only modification you are required to make is adapting the `scatter_plot` function call accordingly to the signature you set in your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YOA8SBsaBFL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "class DensityEstimator:\n",
        "    \"\"\"Class to estimate Density/Distribution of the given data.\n",
        "    1. Write a function to model the distribution of the political party dataset\n",
        "    2. Write a function to randomly sample 10 parties from this distribution\n",
        "    3. Map the randomly sampled 10 parties back to the original higher dimensional\n",
        "    space as per the previously used dimensionality reduction technique.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, data: pd.DataFrame, dim_reducer, high_dim_feature_names, seed=42\n",
        "    ):\n",
        "        self.data = data\n",
        "        self.dim_reducer_model = dim_reducer.model\n",
        "        self.feature_names = high_dim_feature_names\n",
        "        self.seed = seed\n",
        "        self.model = self._model()\n",
        "\n",
        "    ##### YOUR CODE GOES HERE #####\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gak2Olqteu_L"
      },
      "outputs": [],
      "source": [
        "def plot_density_estimation_results(\n",
        "    X: pd.DataFrame,\n",
        "    Y_: np.ndarray,\n",
        "    means: np.ndarray,\n",
        "    covariances: np.ndarray,\n",
        "    title: str,\n",
        "):\n",
        "    \"\"\"Use this function to plot the estimated distribution\"\"\"\n",
        "    color_iter = cycle([\"navy\", \"c\", \"cornflowerblue\", \"gold\", \"darkorange\", \"g\"])\n",
        "    pyplot.figure()\n",
        "    splot = pyplot.subplot()\n",
        "    for i, (mean, covar, color) in enumerate(zip(means, covariances, color_iter)):\n",
        "        v, w = np.linalg.eigh(covar)\n",
        "        v = 2.0 * np.sqrt(2.0) * np.sqrt(v)\n",
        "        u = w[0] / np.linalg.norm(w[0])\n",
        "        if not np.any(Y_ == i):\n",
        "            continue\n",
        "        ##### YOUR CODE GOES HERE #####\n",
        "\n",
        "        ###############################\n",
        "        angle = np.arctan(u[1] / u[0])\n",
        "        angle = 180.0 * angle / np.pi\n",
        "        ell = mpl.patches.Ellipse(mean, v[0], v[1], 180.0 + angle, color=color)\n",
        "        ell.set_clip_box(splot.bbox)\n",
        "        ell.set_alpha(0.5)\n",
        "        splot.add_artist(ell)\n",
        "    pyplot.title(title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PtiA8vEHbC4"
      },
      "source": [
        "## 3.1. Model training\n",
        "> Execute training for the density estimation model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUFN8m3LHbC5"
      },
      "outputs": [],
      "source": [
        "##### YOUR CODE GOES HERE #####\n",
        "\n",
        "###############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN7AsvnaHbC5"
      },
      "source": [
        "## 3.2. Sampling from the distribution\n",
        "\n",
        "> Draw 1,000 samples from your density estimation model and then project them back to the original space of the data.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXQUvVVDHbC5"
      },
      "outputs": [],
      "source": [
        "##### YOUR CODE GOES HERE #####\n",
        "\n",
        "###############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N81rQo9PHbC5"
      },
      "source": [
        "## 3.3. Label the party data\n",
        "\n",
        "> Label the original data according to the estimated density components. Also label the projected data.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymXlOGf6HbC5"
      },
      "outputs": [],
      "source": [
        "##### YOUR CODE GOES HERE #####\n",
        "\n",
        "###############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "absDkAgIHbC5"
      },
      "source": [
        "## 3.4. Plot the confidence regions\n",
        "\n",
        "> Plot the estimated confidence regions for each of the components of the trained density estimator alongside the real (projected) data in order to have an intuition about the model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCGKgQQ56-R8"
      },
      "outputs": [],
      "source": [
        "##### YOUR CODE GOES HERE #####\n",
        "\n",
        "###############################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ul2zX_nHnwM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}